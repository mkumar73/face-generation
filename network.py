from __future__ import division
import os
import time
import math
import tensorflow as tf
import numpy as np
from six.moves import xrange

from helpers import *


class FACE_COMPLETION(object):
    def __init__(self, sess, image_size=64, batch_size=64, 
                 z_dim=100, gf_dim=16, df_dim=16, c_dim=3,
                 checkpoint_dir=None, lam=0.1):
        
        """
        Args:
            sess: TensorFlow session
            batch_size: The size of batch. Should be specified before training.
            z_dim: (optional) Dimension of dim for Z. [100]
            gf_dim: (optional) Dimension of generator filters in first conv layer. [16]
            df_dim: (optional) Dimension of discriminator filters in first conv layer. [16]
            c_dim: (optional) Dimension of image color. [3]
            lam: (optional) scalar to calculate the complete loss from perpetual loss
        """

        self.sess = sess
        self.batch_size = batch_size
        self.image_size = image_size
        self.train_size = np.inf
        self.image_shape = [image_size, image_size, c_dim]

        # beta1 value for training case
        self.beta1_train = 0.5
        
        # some predefined parameters for face completion step
        self.beta1_complete = 0.9
        self.beta2_complete = 0.999
        self.lr_complete = 0.01
        self.approach_complete = 'adam'
        self.eps_complete = 1e-8
        self.centerScale = 0.25

        self.z_dim = z_dim

        self.gf_dim = gf_dim
        self.df_dim = df_dim

        self.lam = lam

        self.c_dim = c_dim

        self.checkpoint_dir = checkpoint_dir
        
        # model building
        self.build_model()

        self.model_name = "fc.model"


    def build_model(self):
        # boolean placeholder to capture if training the model or not
        self.is_training = tf.placeholder(tf.bool, name='is_training')
        
        # placeholder for real images [?, 64, 64, 3]
        self.images = tf.placeholder(
                        tf.float32, [None] + self.image_shape, name='real_images')
        
        # placeholder for variable generated by Generator
        self.z = tf.placeholder(tf.float32, [None, self.z_dim], name='z')
        # capture the summary for random variable
        self.z_sum = tf.summary.histogram("z", self.z)

        self.G = self.generator(self.z)
         
        # get the activation and logits from discriminator
        # layers:  3 hidden layers filter size 16, 32, 64 of kernel shape 5x5
        self.D, self.D_logits = self.discriminator(self.images)

        # get the activation and logits for generator as well
        self.D_, self.D_logits_ = self.discriminator(self.G, reuse=True)

        # accumulate summaries
        self.d_sum = tf.summary.histogram("d_actual", self.D)
        self.d__sum = tf.summary.histogram("d_fake", self.D_)
        self.G_sum = tf.summary.image("Gen", self.G)

        # real loss for discriminator, need to predict 1 for real
        self.d_loss_real = tf.reduce_mean(
                                        tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_logits,
                                        labels=tf.ones_like(self.D)))
        
        # fake loss for discriminator, need to predict 0 for fake images        
        self.d_loss_fake = tf.reduce_mean(
                                        tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_logits_,
                                        labels=tf.zeros_like(self.D_)))
        
        # loss for generator, need to predict 1 for fake images        
        self.g_loss = tf.reduce_mean(
                                    tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_logits_,
                                    labels=tf.ones_like(self.D_)))

        # accumulate summaries for real and fake loss
        self.d_loss_real_sum = tf.summary.scalar("d_loss_real", self.d_loss_real)
        self.d_loss_fake_sum = tf.summary.scalar("d_loss_fake", self.d_loss_fake)

        # total loss of discriminator (real + fake)
        self.d_loss = self.d_loss_real + self.d_loss_fake

        # accumulate summaries for generators and discriminators loss
        self.g_loss_sum = tf.summary.scalar("g_loss", self.g_loss)
        self.d_loss_sum = tf.summary.scalar("d_loss", self.d_loss)

        # get the list of all trainable variables
        t_vars = tf.trainable_variables()

        # store dicriminators and generators variable seperately as list
        self.d_vars = [var for var in t_vars if 'd_' in var.name]
        self.g_vars = [var for var in t_vars if 'g_' in var.name]

        
        # instantiate saver class from tf for storing the checkpoints 
        self.saver = tf.train.Saver(max_to_keep=1)

        
        # Image completion
        # placeholder for the mask [random and center mask only]
        self.mask = tf.placeholder(tf.float32, self.image_shape, name='mask')
        
        # calculate contextual loss
        self.contextual_loss = tf.reduce_sum(
            tf.contrib.layers.flatten(
                tf.abs(tf.multiply(self.mask, self.G) - tf.multiply(self.mask, self.images))), 1)
        
       # perceptual loss is loss by generator 
        self.perceptual_loss = self.g_loss

        # complete loss is fraction of perceptual and contextual loss
        self.complete_loss = self.contextual_loss + self.lam*self.perceptual_loss
        self.grad_complete_loss = tf.gradients(self.complete_loss, self.z)


    def train(self, config):
        # get the data for training
        data = image_files(config.dataset)

        # randomly shuffle the data for better generalization
        np.random.shuffle(data)

        # use Adam optimizer for training dicriminator and generator
        # use the g_vars and d_vars seperately
        # minimize total loss of discriminator (real + fake)
        d_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=self.beta1_train) \
                          .minimize(self.d_loss, var_list=self.d_vars)
        
        # minimize generators loss  
        g_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=self.beta1_train) \
                          .minimize(self.g_loss, var_list=self.g_vars)                

        # initialize all global variables
        tf.global_variables_initializer().run()

        # merge summaries for discrimininator and generator
        self.g_sum = tf.summary.merge(
                                [self.z_sum, self.d__sum, self.G_sum, self.d_loss_fake_sum, self.g_loss_sum])
        self.d_sum = tf.summary.merge(
                                [self.z_sum, self.d_sum, self.d_loss_real_sum, self.d_loss_sum])
        
        # write the summary to log directory
        self.writer = tf.summary.FileWriter("./logs", self.sess.graph)

        counter = 1
        start_time = time.time()

        # check the existence of checkpoints in checkpoint directory
        if self.load(self.checkpoint_dir):
            print("""
********************************************************
An existing model was found in the checkpoint directory.
********************************************************

""")
        else:
            print("""
********************************************************
No pretrained model was found in the checkpoint directory.
Starting from scratch. !! 
********************************************************

""")

        print('Training started........')

        for epoch in xrange(config.epoch):
            # get the images from the dataset directory.
            data = image_files(config.dataset)


            # get the batch_idx to train the model on small batches instead of 100,000 images. 
            batch_idxs = min(len(data), self.train_size) // self.batch_size
            print('Epoch:[{:2d}/{:2d}]'.format(epoch+1, config.epoch))


            for idx in xrange(0, batch_idxs):
                # get images for every batch index incremently.
                batch_files = data[idx*config.batch_size:(idx+1)*config.batch_size]
                
                # get the image as a list
                batch = [get_image(batch_file) for batch_file in batch_files]
                
                # store the images in the form of numpy array.
                batch_images = np.array(batch).astype(np.float32)

                # randomly created the input for generator.
                batch_z = np.random.uniform(-1, 1, [config.batch_size, self.z_dim]).astype(np.float32)

                # Update Discriminator network
                _, summary_str = self.sess.run([d_optim, self.d_sum],
                                            feed_dict={ self.images: batch_images, self.z: batch_z, self.is_training: True })
                # add summary for discriminator
                self.writer.add_summary(summary_str, counter)

                # Update Generator network
                _, summary_str = self.sess.run([g_optim, self.g_sum],
                                            feed_dict={ self.z: batch_z, self.is_training: True })
                # add summary for generator
                self.writer.add_summary(summary_str, counter)
                

                # To avoid the fast convergence of D (discriminator) network, 
                # G (generator) network is updated twice for each D network update.
                _, summary_str = self.sess.run([g_optim, self.g_sum],
                                            feed_dict={ self.z: batch_z, self.is_training: True })
                #  add summary for generator
                self.writer.add_summary(summary_str, counter)

                # calculate the fake loss for discriminator using images generated by generator 
                errD_fake = self.d_loss_fake.eval({self.z: batch_z, self.is_training: False})

                # calculate the actual loss for discriminator using real images 
                errD_real = self.d_loss_real.eval({self.images: batch_images, self.is_training: False})
                
                # calculate the loss for generator using images generated by generator
                errG = self.g_loss.eval({self.z: batch_z, self.is_training: False})

                # add the real and fake loss for discriminator for display purpose
                d_loss_print = errD_fake+errD_real

                counter += 1
                print("Iteration: [{:4d}/{:4d}] time: {:4.4f}, Discriminators_loss: {:.8f}, Generators_loss: {:.8f}".format(
                                                    idx+1, batch_idxs, time.time() - start_time, d_loss_print, errG))               

                if np.mod(counter, 500) == 2:
                    self.save(config.checkpoint_dir, counter)


    def discriminator(self, image, reuse=False):
        """define a function for dicriminator to predict 1 for real and 0 for fake images.

        Args:
            image: image as an input, real and fake both.
            resuse: used for variable scope usage.
        
        Return: Sigmoid activation result and weights of last layer.
        """        
        with tf.variable_scope("discriminator") as scope:
            if reuse:
                scope.reuse_variables()

            # 1st conv layer : input [?, 64, 64, 3], filter [5, 5, 3, 16], output [?, 64, 64, 16] 
            # apply batch normalization and leaky relu non-linearity
            h0 = conv2d(image, self.df_dim, name='d_h0_conv')
            h0 = batch_norm(h0, self.is_training, name='d_bn0')
            h0 = lrelu(h0)

            # 2nd conv layer : input [?, 64, 64, 16], filter [5, 5, 16, 32], output [?, 64, 64, 32] 
            # apply batch normalization and leaky relu non-linearity
            h1 = conv2d(h0, self.df_dim*2, name='d_h1_conv')
            h1 = batch_norm(h1, self.is_training, name='d_bn1')
            h1 = lrelu(h1)

            # 3rd conv layer : input [?, 64, 64, 32], filter [5, 5, 32, 64], output [?, 64, 64, 64]
            # apply batch normalization and leaky relu non-linearity
            h2 = conv2d(h1, self.df_dim*4, name='d_h2_conv')
            h2 = batch_norm(h2, self.is_training, name='d_bn2')
            h2 = lrelu(h2)

            # 4th conv layer : input [?, 64, 64, 64], filter [5, 5, 64, 128], output [?, 64, 64, 128]
            # apply batch normalization and leaky relu non-linearity
            h3 = conv2d(h2, self.df_dim*8, name='d_h3_conv')
            h3 = batch_norm(h3, self.is_training, name='d_bn3')
            h3 = lrelu(h3)            

            # fully connected layer : input [-1, 64*64*128], output [?, 64]
            h4 = flatten(h3)
            h4 = linear(h4, 1, 'd_h4_lin', with_w=False)

            return tf.nn.sigmoid(h4), h4


    def generator(self, z):
        """define a function to generate images from a random sample.

        Args:
            z: random sample of size 100.
        
        Return: Activation result as an upscaled image of size [64, 64, 3]
        """
        with tf.variable_scope("generator") as scope:

            # 1st generator layer
            # apply batch normalization and relu non-linearity
            self.z_ = linear(z, self.gf_dim*8*4*4, 'g_h0_lin', with_w=False)
            hs0 = batch_norm(self.z_, self.is_training, name='g_bn0', is_linear=True)
            hs0 = relu(hs0)
            hs0 = tf.reshape(self.z_, [-1, 4, 4, self.gf_dim * 8])

            # 2nd layer: : input [?, 4, 4, 128], filter [5, 5, 64, 128], output [?, 8, 8, 64]
            # apply batch normalization and relu non-linearity
            hs1 = conv2d_transpose(hs0, [self.batch_size, 8, 8, 64], name='g_h1_deconv', with_w=False)
            hs1 = batch_norm(hs1, self.is_training, name='g_bn1')
            hs1 = relu(hs1)

            # 3rd layer: : input [?, 8, 8, 64], filter [5, 5, 32, 64], output [?, 16, 16, 32]
            # apply batch normalization and relu non-linearity
            hs2 = conv2d_transpose(hs1, [self.batch_size, 16, 16, 32], name='g_h2_deconv', with_w=False)
            hs2 = batch_norm(hs2, self.is_training, name='g_bn2')
            hs2 = relu(hs2)            

            # 4th layer: : input [?, 16, 16, 32], filter [5, 5, 16, 32], output [?, 32, 32, 16]
            # apply batch normalization and relu non-linearity
            hs3 = conv2d_transpose(hs2, [self.batch_size, 32, 32, 16], name='g_h3_deconv', with_w=False)
            hs3 = batch_norm(hs3, self.is_training, name='g_bn3')
            hs3 = relu(hs3)            

            # 5th layer: : input [?, 32, 32, 16], filter [5, 5, 3, 16], output [?, 64, 64, 3]
            # apply batch normalization and tanh non-linearity
            hs4 = conv2d_transpose(hs3, [self.batch_size, 64, 64, 3], name='g_h4_deconv', with_w=False)
            # hs4 = batch_norm(hs4, self.is_training, name='g_bn4')
            hs4 = tf.nn.tanh(hs4)             

            return hs4


    def complete(self, config):
        """define a function to complete masked image.

        Args:
            config: User specified configuration, input from command line.
        
        Return: Save the completed masked image to output directory.
        """        

        # initialize variables.
        tf.global_variables_initializer().run()

        # load the generator trained model 
        is_loaded = self.load(self.checkpoint_dir)
        assert(is_loaded)

        # center masking
        if config.mask_type == 'center':
            assert(self.centerScale <= 0.5)
            mask = np.ones(self.image_shape)
            sz = self.image_size
            l = int(self.image_size * self.centerScale)
            u = int(self.image_size * (1.0-self.centerScale))
            mask[l:u, l:u, :] = 0.0

        else:
            assert(False)
            print('Only center masking can be done..!!')

        # run the face completion iteratively to improve result
        # by testing, it is seen that anything more than two round is sufficient for image completion
        img_count = len(config.imgs)
        batch_idxs = min(int(np.ceil(img_count/self.batch_size)), 2)

        for idx in xrange(0, batch_idxs):
            print('Round: {0}/{1}'.format(idx+1,batch_idxs))
            l = idx*self.batch_size
            u = min((idx+1)*self.batch_size, img_count)
            batchSz = u-l
            batch_files = config.imgs[l:u]
            batch = [get_image(batch_file) for batch_file in batch_files]
            batch_images = np.array(batch).astype(np.float32)

            zhats = np.random.uniform(-1, 1, size=(self.batch_size, self.z_dim))
            m = 0
            v = 0

            nRows = np.ceil(batchSz/8)
            nCols = min(8, batchSz)
            save_images(batch_images[:batchSz,:,:,:], [nRows,nCols], os.path.join(config.out_dir, 'original.png'))
            
            masked_images = np.multiply(batch_images, mask)
            save_images(masked_images[:batchSz,:,:,:], [nRows,nCols], os.path.join(config.out_dir, 'masked.png'))


            for i in xrange(config.num_iter):
                fd = {
                    self.z: zhats,
                    self.mask: mask,
                    self.images: batch_images,
                    self.is_training: False
                }

                run = [self.complete_loss, self.grad_complete_loss, self.G]
                loss, g, G_imgs = self.sess.run(run, feed_dict=fd)


                if i % config.out_interval == 0:
                    print('Iteration: {}, Face completion loss: {}'.format(i, np.mean(loss[0:batchSz])))
                    
                    inv_masked_hat_images = np.multiply(G_imgs, 1.0-mask)
                    completed = masked_images + inv_masked_hat_images
                    imgName = os.path.join(config.out_dir,'{:04d}.png'.format(i))
                    save_images(completed[:batchSz,:,:,:], [nRows,nCols], imgName)

                if self.approach_complete == 'adam':
                    # Optimize single completion with Adam
                    m_prev = np.copy(m)
                    v_prev = np.copy(v)
                    m = self.beta1_complete * m_prev + (1 - self.beta1_complete) * g[0]
                    v = self.beta2_complete * v_prev + (1 - self.beta2_complete) * np.multiply(g[0], g[0])
                    m_hat = m / (1 - self.beta1_complete ** (i + 1))
                    v_hat = v / (1 - self.beta2_complete ** (i + 1))
                    zhats += - np.true_divide(self.lr_complete * m_hat, (np.sqrt(v_hat) + self.eps_complete))
                    zhats = np.clip(zhats, -1, 1)

                else:
                    assert(False)


    def save(self, checkpoint_dir, step):
        """define a function to save the checkpoint files.

        Args:
            checkpoint_dir: path for the checkpoint directory.
            step: counte value after which checkpoints will be stored.

        """
        if not os.path.exists(checkpoint_dir):
            os.makedirs(checkpoint_dir)

        self.saver.save(self.sess,
                        os.path.join(checkpoint_dir, self.model_name),
                        global_step=step)


    def load(self, checkpoint_dir):
        """define a function to load the existing checkpoint files.

        Args:
            checkpoint_dir: path for the checkpoint directory.

        """
        print(" ****** Reading checkpoints... ******")

        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)
        if ckpt and ckpt.model_checkpoint_path:
            self.saver.restore(self.sess, ckpt.model_checkpoint_path)
            return True
        else:
            return False


